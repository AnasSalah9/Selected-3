# -*- coding: utf-8 -*-
"""Selected 3 Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14YjuWwjSNwePF8L-PZy3lpWCiJV8WBXE
"""

import sys
import requests
import time
import csv
from bs4 import BeautifulSoup

!pip install selenium
!apt-get update # to update ubuntu to correctly run apt install
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')

from selenium import webdriver
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC

def all_pages_links():
  """
  This function returns a list of 20 urls, a url for each page
  of the 20 pages of the Authors section in the website
  """
  url_list = []
  url_list.append("https://www.arab-books.com/%D8%A7%D9%84%D9%85%D8%A4%D9%84%D9%81%D9%88%D9%86/")

  while True:
    response = requests.get(url_list[-1])
    data = response.text
    soup = BeautifulSoup(data, 'html.parser')
    next_page = soup.find('a', {'class':'next page-numbers'})

    if next_page:
      url_list.append(next_page.get('href'))
    else:
      break

  return url_list

def pageTuples(url):
  """
  This function takes a url to one of the 20 pages that contains multiple authors pages, and returns a list of tuples
  [(AuthorName, BookName, BookLink, BookCategory, Language, NumberOfPages, Publisher, BookSize, BookExtention, dummy)]
  """
  page_number = url[-1]

  response = requests.get(url)
  data = response.text
  soup = BeautifulSoup(data, 'html.parser')
  authors = soup.find_all('li', {'class':'post-item'})

  chrome_options = webdriver.ChromeOptions()
  chrome_options.add_argument('--headless')
  chrome_options.add_argument('--no-sandbox')
  chrome_options.add_argument('--disable-dev-shm-usage')
  driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)

  page_rows = []
  for author in authors:
    author_name = author.find('h3', {'class':'post-title'}).text
    books_link = author.find('a').get('href')
  
    driver.get(books_link)
  
    # if there is load-more button this code presses that button
    try:
      button = driver.find_element(by=By.ID, value="load-more-archives")
      count = button.get_attribute('data-max')
      count = int(count)
      #print("@@@ ", count, " @@@")
      for i in range(count):
        button.click()
        time.sleep(2)
    except:
      print("@@ No load-more button @@")


    books_data = driver.page_source
    books_soup = BeautifulSoup(books_data)
    books =  books_soup.find('ul', {'id':'posts-container'})
    books = books.find_all('li') if books else []
    

    for book in books:
      #author_name, book_name, book_download_link, a1, a2, a3, a4, a5, a6, a7, a8

      book_name = book.find('h3', {'class':'post-title'}).text
      book_download_link = book.find('h3', {'class':'post-title'}).find('a').get('href')
      #print(book_name)
      #print(book_download_link)

      print("\n>>>> ", page_number, "\n>>>> ", author_name, "\n>>>> ", book_name, "\n")
      
      try:
        book_response = requests.get(book_download_link)
      except:
        continue

      book_data = book_response.text
      book_soup = BeautifulSoup(book_data, 'html.parser')
      book_attributes = book_soup.find('div', {'class':'book-info'})
      book_attributes = book_attributes.find_all('li')
      book_pdf_link = book_soup.find('div', {'class':'read-link-bottom'})
      book_pdf_link = book_pdf_link.find('a').get('href') if book_pdf_link else "N/A"
      
      book_description = book_soup.find('di')

      if book_pdf_link != "N/A":
        pdf_response = requests.get(book_pdf_link)
        pdf_data = pdf_response.text
        pdf_soup = BeautifulSoup(pdf_data, 'html.parser')
        pdf_link = pdf_soup.find('iframe', {'class':'pdfjs-viewer'})
        pdf_link = pdf_link.get('src') if pdf_link else "N/A" 
      else:
        pdf_link = "N/A"

      #print(pdf_link)

      att = []
      for attribute in book_attributes: att.append(attribute.text)
      row = (author_name,
             book_name,
             pdf_link,
             att[0], att[1], att[2], att[3], att[4], att[5], att[6], att[7]
             )

      page_rows.append(row)
    
  return page_rows

# create a csv file for each page (20 csv files) for debugging purposes
urls = all_pages_links()
page_tuples = []
count = 0
for url in urls:
  count += 1
  page_tuples = pageTuples(url)
  with open('file'+str(count)+'.csv','w') as out:
    csv_out=csv.writer(out)
    csv_out.writerow(['Author_Name', 'Book_Name', 'Book_Link',
                      'Author', 'Book_Category', 'Book_Language', 
                      'Number_Of_Pages', 'Publisher', 'Book_Size', 
                      'Book_extention', 'Dummy'
                      ])
    csv_out.writerows(page_tuples)

# create one csv file for all pages  
urls = all_pages_links()
all_tuples = []

for url in urls:
  all_tuples.append(pageTuples(url))

with open('totalFile.csv','w') as out:
    csv_out=csv.writer(out)
    csv_out.writerow(['Author_Name', 'Book_Name', 'Book_Link',
                      'Author', 'Book_Category', 'Book_Language', 
                      'Number_Of_Pages', 'Publisher', 'Book_Size', 
                      'Book_extention', 'Dummy'
                      ])
    csv_out.writerows(all_tuples)

from google.colab import files
uploaded = files.upload()







